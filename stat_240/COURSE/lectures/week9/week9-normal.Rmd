---
title: "Normal Distribution"
output: html_document
---

\newcommand{\E}{\mathsf{E}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\SD}{\mathsf{SD}}
\renewcommand{\prob}{\mathsf{P}}


This R Markdown document includes contributions from Professor Bret Larget

### Setup details

- You will need the package `tidyverse` for these lectures.   
- You will need the **egg** package for this file
    - Install **egg** if you do not already have it
    
- Later lectures use the following scripts, assumed to be in your course scripts directory.
    - `COURSE/scripts/viridis.R`
    - `COURSE/scripts/ggprob.R`

- The file `ggprob.R` contains a number of functions for graphing probability distributions in a tidyverse-friendly manner.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      error = TRUE, fig.height = 4)
library(tidyverse)
library(kableExtra)
library(egg)
library(broman)
source("../../scripts/viridis.R")
source("../../scripts/ggprob.R")
```


## Normal Distributions

## Overview

- Normal distributions are continuous distributions

- All normal distributions have the same overall shape:  symmetric, unimodal, bell-shaped

- The shape of the normal density curve is completely determined by its mean ($\mu$) and its standard deviation ($\sigma$)

- The density function has the following formula

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

- The mean $\mu$ is located at the center and the standard deviation $\sigma$ controls the spread.

```{r normal-density}
mu = 3
sigma = 0.5
mu2 = 7
sigma2 = 2

gnorm(mu=mu, sigma=sigma, a=-0, b=12, color="blue",
      fill=NULL, title=FALSE) +
  geom_norm_density(mu = mu2, sigma = sigma2, color = "magenta") +
  theme_minimal()
```

- Note that the commands `gnorm()` and `geom_norm_density()` are defined in the file `ggprob.R` and are not part of the **tidyverse**.

## Normal Probabilities

- Suppose that $X \sim \text{N}(\mu, \sigma)$
    - this means the random variable $X$ follows a normal distribution with mean $\mu$ and standard deviation $\sigma$.

- We can calculate probabilities for a Normal distribution using R:

$$P(a \leq X \leq b)$$

- This is the area under the density curve between $b$ and $a$
- The function `pnorm()` calculates areas under normal curves.

- The arguments in order are: 
    - `q`: a location on the x axis
    - `mean`: the mean of the distribution, or $\mu$, default value is 0
    - `sd`: the standard deviation of the distribution, or $\sigma$, default value is 1
    - `lower.tail`: logical to calculate area to the left, default value is `TRUE`
    - `log.p`: logical to calculate log of the probability, default value is `FALSE`.
    
### Examples:

#### Area to the left

- Find $\prob(X < 80)$

```{r}
## naming arguments; I usually do not
pnorm(q=80, mean=100, sd=25)
## without the names
pnorm(80, 100, 25)

## gnorm is defined in ggprob.R
## you need to call source("../../scripts/ggprob.R") in set-up for this to work
## a is the left endpoint of the interval to fill
## b is the right endpoint of the interval to fill
## use NULL for the default value (all the way to the left or right)
gnorm(mu = 100, sigma = 25) +
  geom_norm_fill(mu = 100, sigma=25, a = NULL, b = 80) +
  theme_minimal()
```


#### Area to the right

- Find $\prob(X > 90)$

```{r}
## using lower.tail
pnorm(90, 100, 25, lower.tail = FALSE)
## using subtraction
1 - pnorm(90, 100, 25)

## first argumment is mu, second is sigma if you do not want to specify names
## Need to repeat mu and sigma in each level: not set in aes()
gnorm(100, 25) +
  geom_norm_fill(100, 25, a = 90, b = NULL) +
  theme_minimal()
```


#### Area between two values

- Find $\prob(50 < X < 120)$
    - Note: $\prob(50 < X < 120) = \prob(X < 120) - \prob(X < 50)$

```{r}
## use subtraction
pnorm(120, 100, 25) - pnorm(50, 100, 25)

gnorm(100, 25) +
  geom_norm_fill(100, 25, a = 50, b = 120) +
  theme_minimal()
```


#### Two tail areas

- Find $\prob(|X-100| > 30)$
    - This is $\prob(X < 70) + \prob(X > 130)$

```{r}
## using lower.tail
pnorm(70, 100, 25) + pnorm(130, 100, 25, lower.tail = FALSE)
## using subtraction
pnorm(70, 100, 25) + (1 - pnorm(130, 100, 25))

gnorm(100, 25) +
  geom_norm_fill(100, 25, a = NULL, b = 70) +
  geom_norm_fill(100, 25, a = 130, b = NULL) +
  theme_minimal()
```



## Standard Normal Distribution and Standardization

- A *standard normal distribution* is a normal distribution with mean = 0 and standard deviation = 1

```{r}
gnorm() +
  theme_minimal()
```


-  If $X \sim \text{N}(\mu, \sigma)$, then $Z = \left(\frac{X-\mu}{\sigma}\right) \sim \text{N}(0, 1)$

- An implication is $\prob(X < a) = \prob\left( Z < \frac{a-\mu}{\sigma}\right)$

#### Example

$X \sim \text{N}(100, 25)$

- Find $\prob(X < 80)$

```{r}
mu = 100
sigma = 25
x = 80
z = (x-mu)/25

## Using X directly
pnorm(x, mu, sigma)

## Standardizing first
## default values for the mean and sd are 0 and 1
pnorm(z)

gnorm(mu, sigma) +
  geom_norm_fill(mu, sigma, b=x) +
  theme_minimal()

gnorm() +
  geom_norm_fill(b=z) +
  theme_minimal()
```

- Notice the graphs are identical except for axis labels
- All normal curves have exactly the same shape, but they may differ in center and scale

## Normal Quantiles

- The p-quantile is the location `q` where the area to the left of `q` under the normal density is equal to `p`.

- The R function `qnorm()` makes these calculations.
- Arguments in order are:
    - `p`: the probability of the quantile
    - `mean`: mean of the distribution, $\mu$, default is 0
    - `sd`: standard deviation of the distribution, $\sigma$, default is 1
    - `lower.tail`: default is `TRUE`, if `FALSE` find locations for areas to the right  
    - `log.p`: default is `FALSE`, if true, it returns natural log of the quantile
    
### Examples

$X \sim \text{N}(100, 25)$

Find the 0.1, 0.25, 0.9, 0.95, 0.975, and 0.99 quantiles of the distribution.

```{r}
p = c(0.1, 0.25, 0.9, 0.95, 0.975, 0.99)
mu = 100
sigma = 25

qnorm(p, mu, sigma)
```

### Graphs

```{r, fig.height = 12}
## use ggarrange() to display multiple plots and map() to create a list of plots

normal_plots = p %>% 
  map(~{
    gnorm(mu, sigma) +
    geom_norm_fill(mu, sigma, a = NULL, b = qnorm(.x, mu, sigma)) +
    theme_minimal()
  })

ggarrange(plots = normal_plots,
          nrow = length(p),
          ncol = 1)
```

### Simple Normal Calculation example

> The weights of packets of cookies produced by a certain manufacturer have a normal distribution with a mean of 202 g and a standard deviation of 3 g. What is the weight that should be labeled on the packet so that only 1% of the packets are underweight?

- $X \sim \text{N}(202, 3)$, want $P(X < a) = 0.01$  

```{r simple-normal}
## This gives the 1st percentile of a N(202, 3) distribution
qnorm(0.01, mean=202, sd=3, lower.tail=TRUE)

## Check
pnorm(195.021, mean=202, sd=3) # P(X <= 195.021) = .01
```


## Normal approximation to the binomial distribution

- If $n$ is large enough, and $p$ is not too close to 0 or 1, the binomial distribution can be approximated by the normal distribution 
- A rule of thumb for "large enough" is if $np(1-p) \geq 10$.
    - Under the assumptions above, if $X \sim \text{Binomial}(n,p)$, then $X \stackrel{\text{approx}}{\sim}\text{N}(np, \sqrt{np(1-p)})$
    

- Here is an example where the assumption is satisfied.
    - $n = 100$
    - $p = 0.5$
    - $np(1-p) = 25$
    
```{r normal-approx-good}
## Assumptions satisfied
n = 100
p = 0.5
mu = n*p
sigma = sqrt(n*p*(1-p))

gbinom(n, p, scale=TRUE) +
  geom_norm_density(mu, sigma, color = "red") +
  theme_minimal()
```



### Example where the approximation is poor

- Here is an example where the assumption is **not satisfied**.
    - $n = 100$
    - $p = 0.01$
    - $np(1-p) = 0.99$
    
```{r normal-approx-bad}
## Criteria not satisfied
n = 100
p = 0.01
mu = n*p
sigma = sqrt(n*p*(1-p))

gbinom(n, p, scale=TRUE) +
  geom_norm_density(mu, sigma, color = "red") +
  theme_minimal()
```

### Correction for Continuity

- If we want to calculate a binomial probability, we should just do so directly

- However, if we do want to use a normal approximation, your calculation will be more accurate if add or subtract 0.5 as appropriate.

- The exact binomial probability that $X=x$ is approximated by the area under a normal density curve between $x-0.5$ and $x+0.5$.

- So, for example, when $n = 20$ and $p = 0.4$, the exact binomial probability $\prob(X \le 6)$ and the normal approximation.

```{r}
## Exact calculation
pbinom(6, 20, 0.4)
```

- For the normal approximation, note that $\prob(X \le 6) = \prob(X < 7)$.
  - The area under a normal curve to the left of 6 misses part of the binomial probability exactly at 6 and is too small.
  - The area under a normal curve to the left of 7 includes part of the binomial probability exactly at 7 and is too big.
  
```{r}
## Normal approximations without correction

## moments
mu = 20*0.4
sigma = sqrt(20*0.4*0.6)

## approximations
pnorm(6, mu, sigma)
pnorm(7, mu, sigma)
```

- A better approximation uses 6.5 as the endpoint for the normal approximation.

```{r}
pnorm(6.5, mu, sigma)
```

- But the exact calculation is the most accurate, of course.

```{r}
cc_example = tibble(
  x = 0:20,
  p = dbinom(x, 20, 0.4)
)
```

- Approximation too small

```{r}
ggplot(cc_example, aes(x = x, y = p)) +
  geom_col(fill = "gray") +
  geom_norm_fill(mu = mu, sigma = sigma, b = 6, alpha = 0.5) +
  geom_norm_density(mu, sigma, color = "blue") +
  geom_hline(yintercept = 0) +
  theme_minimal()
```

- Approximation too large

```{r}
ggplot(cc_example, aes(x = x, y = p)) +
  geom_col(fill = "gray") +
  geom_norm_fill(mu = mu, sigma = sigma, b = 7, alpha = 0.5) +
  geom_norm_density(mu, sigma, color = "blue") +
  geom_hline(yintercept = 0) +
  theme_minimal()
```

- Approximation better

```{r}
ggplot(cc_example, aes(x = x, y = p)) +
  geom_col(fill = "gray") +
  geom_norm_fill(mu = mu, sigma = sigma, b = 6.5, alpha = 0.5) +
  geom_norm_density(mu, sigma, color = "blue") +
  geom_hline(yintercept = 0) +
  theme_minimal()
```


---
title: "Probability and Simulation"
output: html_document
---

\newcommand{\E}{\mathsf{E}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\SD}{\mathsf{SD}}
\renewcommand{\prob}{\mathsf{P}}

### Setup details

- You will need the package **tidyverse** for this file
- Also, install the **broman** package for the `myround()` function which actually rounds output to the requested number of digits (as a string).

- This lecture use the following scripts, assumed to be in your course scripts directory.
    - `COURSE/scripts/viridis.R`
    - `COURSE/scripts/ggprob.R`

- The file `ggprob.R` contains a number of functions for graphing probability distributions in a tidyverse-friendly manner.


## Simulation and Probability

- For many well-known distributions such as the binomial and normal distributions, there is a combination of theory and computational methods that allow us to calculate numerical values for distribution characteristics such as means, variances, and standard deviations,
or the probabilities of various outcomes.
- However, there are often times where either we do not know the theory or the theory does not exist for us to calculate moments or various probabilities.
- Often times, we can use simulation in such instances to estimate these quantities.

### Example 1

- We begin with an example where we have learned a method.
- Suppose that a random variable $X_1 \sim \text{Normal}(250, 30)$ meaning $\E(X) = \mu = 250$ is the mean of the distribution and that the standard deviation is $\sigma = 30$.
- Suppose also we were interested in calculating $\prob(X > 300)$.
- From a previous lecture, we know we just need to find the area to the right of 300 under the normal density for this distribution.
- The base R function `pnorm()` can give us a numerical answer.

```{r}
1 - pnorm(300, 250, 30)
```

- A graph helps to visualize the answer.

```{r}
gnorm(250, 30) +
  geom_norm_fill(250, 30, a = 300)
```

#### Simulation Solution

- Now, let's say we forgot about the existence of `pnorm()`, but we did know how to use `rnorm()` to sample random variables.
- The idea would be to sample many instances random variables with this distribution and merely count the proportion that are larger than 300.

- First, we do a **tidyverse** solution where we create a data frame with the random sample in a column and then use **dplyr** commands to calculate the desired summary.
  - We will use a simulation size of 1,000,000 and repeat a few times to get a sense of the error due to simulation.

```{r}
## setting the seed here so that we get the same values each time we knit
## We will only set the seed once in this file
set.seed(2022)

B = 1000000
prob1 = tibble(
  x1 = rnorm(B, 250, 30),
  x2 = rnorm(B, 250, 30),
  x3 = rnorm(B, 250, 30),
  x4 = rnorm(B, 250, 30))

prob1_prob = prob1 %>% 
  summarize(p1 = mean(x1 > 300),
            p2 = mean(x2 > 300),
            p3 = mean(x3 > 300),
            p4 = mean(x4 > 300))
prob1_prob
```

- It appears that if we round off our answers to four decimal places, different simulations give slightly different answers.
- Rounding to three decimal places, we are reasonably confident that the probability is 0.048.
- Compare again to the exact answer rounded to four decimal places.

```{r}
round(1 - pnorm(300, 250, 30), 4)
```

#### Base R Solution

- We could have repeated the same calculation using base R.
- In this course, we tend to show **tidyverse** solutions most of the time, but knowing how to do similar calculations in base R is helpful.

```{r}
## A base R solution
prob1_base = mean( rnorm(B, 250, 30) > 300 )
round(prob1_base, 4)
```

### Example 2

- Next, let's try another example where we have theory to tell us the exact number.
- Let $X_2 \sim \text{Binomial}(500, 0.2)$ so the mean is $\E(X_2) = 500 \times 0.2 = 100$ and the standard deviation is $\sigma = \sqrt{500(0.2)(0.8)} \doteq `r myround(sqrt(500*0.2*0.8), 2)`$.
- Find $\prob(X \ge 120)$.

- As a review, let's do the calculation exactly first with `pbinom()`.

```{r}
## Notice P(X >= 120) = 1 - P(X <= 119)
prob2 = 1 - pbinom(119, 500, 0.2)
prob2
```

- Here is a simulation based solution

```{r}
B = 1000000
prob2_df = tibble(
  x = rbinom(B, 500, 0.2)
)

prob2_sim = prob2_df %>% 
  summarize(prob2 = mean(x >= 120)) %>% 
  pull(prob2)

prob2_sim
```

- In this example, we get the same value when rounded off to four digits, but that may have been a bit lucky as we might have easily been off in the fourth digit.
  - But the simulation was large enough, it appears, to be highly probable in repeated simulations to have been accurate to three decimal places.
  
### Example 3

- Here is an example that delves into random sampling.
- Suppose that $X_1, X_2, \ldots, X_{10}$ each have a $\text{Normal}(100, 20)$ distribution and that they are mutually independent.
  - Informally, *mutually independent* means that the realized values of any of the random variables have no effect on the probabilities of the values of the others.
- Let $\bar{X}$ be the sample mean, $\bar{X} = (X_1 + \cdots + X_{10})/10$.
- Find $\prob(95 < \bar{X} < 105)$.

- We may write this set-up as $X_i \overset{\text{iid}}{\sim} \text{Normal}(100, 20), \text{ for } i = 1,\ldots,10$.
  - The text "iid" is short for *independent and identically distributed*, a phrase used often when studying probability and mathematical statistics more formally.

#### Solution

- If we had some theory that told us that $\bar{X}$ had a normal distribution with a mean and standard deviation we could calculate,
it would be possible to answer this question with a direct calculation.
- In the absence of this theory, let's do a simulation.
- In this solution, we will use the function `map_dbl()` which is from the **tidyverse** package **purrr** which is used for iteration.
  - The subscript `_dbl` specifies that the return value is a vector of numerical values (double precision floating point values).
- The first argument will be the items we want to iterate over,
in this case a vector of length $B = 1,000,000$.
- The second argument is a function to call.
  - Here, we just need to repeated taking the mean of a random sample of 10 normal random variables which we can do with `mean(rnorm(10, 100, 20))`.
- The code chunk below simulates each sample and calculates and saves the mean, without retaining the individually sampled variables.
  - We save the means of the simulated samples in a single column of a tibble.
  - It takes a few seconds to run.

```{r}
B = 1000000
prob3_df = tibble(
  xbar = map_dbl(1:B, ~mean(rnorm(10, 100, 20))))
```

- Then compute the observed proportion of sample means between 95 and 105.

```{r}
prob3 = prob3_df %>% 
  summarize(p = mean( between(xbar, 95, 105))) %>% 
  pull(p)

prob3
```

- We see that the answer is close to 0.571.

#### Visualize

- We will visualize the answer by graphing an estimated density of the sample in blue, and then overlaying a normal density with the mean and standard deviation which match these calculated summary statistics, and shading the area between 95 and 105.

```{r}
prob3_sum = prob3_df %>% 
  summarize(mu = mean(xbar),
            sigma = sd(xbar))

mu = prob3_sum %>% 
  pull(mu)

sigma = prob3_sum %>% 
  pull(sigma)

mu
sigma
```

- Not surprisingly, the sample mean of the 1,000,000 simulated sample means is close to 100, the mean of each $X_i$.
- The standard deviation is less than $\sigma = 20$ but does not look close to any simple number
  - (More on this soon!)
  
- Here is the plot.

```{r}
ggplot(prob3_df, aes(x = xbar)) +
  geom_density(color = "blue") +
  geom_norm_density(mu, sigma, color = "red") +
  geom_norm_fill(mu, sigma, a = 95, b = 105, fill = "red") +
  geom_hline(yintercept = 0) +
  xlab("Sample Mean") +
  ylab("Density") + 
  ggtitle("Example 3: normal sample mean")
```

#### Observations

- Notice that the estimated density curve is nearly a perfect match for the theoretical normal density with these moments.
- This is not a coincidence:
  - When a random sample of size $n$ is sampled from a normal distribution with mean $\mu$ and standard deviation $\sigma$, the sample mean $\bar{X}$ has a normal distribution with mean $\mu$ and standard deviation $\sigma / \sqrt{n}$.
  - We write this succinctly as $\bar{X} \sim \text{Normal}(\mu, \sigma / \sqrt{n})$
- For our numerical example, $\text{SD}(\bar{X}) = 20 / \sqrt{10} \doteq `r myround(20/sqrt(10), 3)`$ which is very close to our observed sample standard deviation of `r myround(sigma, 3)`.

- If we had used theory and not simulation,
we could have calculated the probability like this.

```{r}
pnorm(105, 100, 20 / sqrt(10)) - pnorm(95, 100, 20 / sqrt(10))
```

### Example 4

- Suppose that the random variables $U_1, \ldots, U_{20}$ are each uniformly distributed between 0 and 10:
$U_i \overset{\text{iid}}{\sim} \text{Uniform}(0,10) \text{ for } i = 1, \ldots, 20$.
- What is the probability that the sum $S_{20} = U_1 + \cdots + U_{20}$ is larger than 115?

#### Solution

- The $\text{Uniform}(0,10)$ distribution has a density equal to 0.1 between 0 and 10 and is 0 elsewhere.

```{r}
unif_0_10 = tibble(x = c(-1, 0, 10, 11),
                   y = c(0, 0.1, 0.1, 0))

ggplot(unif_0_10, aes(x=x, y=y)) +
  geom_step(color = "blue") +
  geom_hline(yintercept = 0) +
  xlab("U") +
  ylab("Density") +
  ggtitle("Uniform(0,10) Distribution")
```

- Let's solve using simulation
    - Generate a large number $B$ of samples of size 20 from this uniform distribution
    - Find the sum of the 20 values in each sample
    - Save these $B$ sample sums
    - Calculate the proportion that are larger than 115.
    
```{r}
B = 1000000

prob4_df = tibble(
  s20 = map_dbl(1:B, ~sum(runif(20, 0, 10))))
```
    
- Estimate the probability

```{r}
p4 = prob4_df %>% 
  summarize(p = mean(s20 > 115)) %>% 
  pull(p)

p4
```

- The probability estimated by simulation is about `r myround(p4, 3)`.

#### Theory

- An exact theoretical calculation is quite difficult.
- But, let's take a look at the estimated density of the sampled sums and compare to a normal distribution where the mean and standard deviation match the observed values.

```{r}
prob4_sum = prob4_df %>% 
  summarize(mu = mean(s20),
            sigma = sd(s20),
            p = mean(s20 > 115))
prob4_sum

mu = prob4_sum$mu
sigma = prob4_sum$sigma

mu
sigma
```

#### Plot

```{r}
ggplot(prob4_df, aes(x = s20)) +
  geom_density(color = "blue") +
  geom_norm_density(mu, sigma, color = "red") +
  geom_norm_fill(mu, sigma, a = 115, fill = "red") +
  geom_hline(yintercept = 0) +
  xlab("Sum") +
  ylab("Density") +
  ggtitle("Sum of Uniforms and Normal Approximation")
```

- Again, the exact probability is well-approximated by an area under a normal density curve because the exact distribution of the sum of 20 uniform random variables is nearly normal.
  - This is not a coincidence!
  
#### Theory

- The exact distribution of the sum of uniform random variables is **not normal** in general.
- But:
  - $\E(S_{20})$ is exactly equal to the sum of the means $\E(U_i)$, or $20 \times 5 = 100$; and
  - $\Var(S_{20})$ is exactly equal to the sum of the variances of the $\{U_i\}$, $\Var(S_{20}) = 20 \times 100/12 \doteq 166.67$, meaning that the exact standard deviation is $\SD(S_{20} = \sqrt{2000/12} \doteq 12.9)$.

- Using the normal approximation,

```{r}
1 - pnorm(115, 100, sqrt(2000/12))
```

- So the simulated probability differed from the theoretical value by about one in the third significant digit, a small relative error.

### Central Limit Theorem

- The past two problems highlight a fundamental result from probability and mathematical statistics called *the central limit theorem*.
- A rigorous statement of the theorem states how the exact probability distribution of a sample mean converges to a normal distribution as the sample size increases to infinity.
- A practical statement is as follows:

> The probability distribution of a sample mean of size $n$ from an iid random sample from a distribution with mean $\mu$ and standard deviation $\sigma$ has mean equal to $\mu$, standard deviation equal to $\sigma/\sqrt{n}$. Furthermore, if $n$ is sufficiently large, this distribution of the sample mean is approximately normal.

- In practice, $n$ need not be all that large.
- If the base distribution is fairly symmetric, then the normal approximation can be very good for small to moderate $n$ (say, a few dozen or less).
- However, if the base distribution is very skewed or has a very high probability on a single value, then $n$ may need to be substantially larger before the normal approximation is accurate.

### Example 5

- Here is an example where the normal approximation fails, but simulation can still obtain an accurate value.

- The *gamma* distribution is the distribution of a positive continuous random variable with a density function equal to
$f(x) = C x^{\alpha-1} \mathrm{e}^{-\lambda x}, \ x > 0$
where $\alpha>0$ and $\lambda>0$ are positive parameters which determine the shape and scale of the distribution, respectively, and $C$ is a constant (depending on the values of $\alpha$ and $\lambda$, but not $x$) so that the total area under the density curve equals one.

- When $\alpha$ is small, the gamma density is strongly skewed to the right.
  - For example, when $\alpha = 0$, the density is the tail of an exponential function.

- The mean of the distribution is $\mu = \alpha / \lambda$ and the variance is $\sigma^2 = \alpha / \lambda^2$ so that the standard deviation is $\sigma = \sqrt{\alpha}/\lambda$.
Here is a graph of the density when $\alpha = 0.9$ and $\lambda = 0.09$.
The mean is $\mu = 0.9 / 0.09 = 10$ and the standard deviation is $\sigma = \sqrt{0.9}/0.09 \doteq `r myround(sqrt(0.9)/0.09, 2)`$.

```{r, echo = FALSE}
tibble(
  x = c(0.01, seq(0.1, 50, 0.1)),
  y = dgamma(x, 0.9, 0.09)) %>% 
ggplot(aes(x=x, y=y)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  xlab("x") + 
  ylab("Density") +
  ggtitle("Gamma(0.9, 0.09) Distribution")
```

#### Problem

- Suppose that $X_1, \ldots, X_4 \overset{\text{iid}}{\sim} \text{Gamma}(0.9, 0.09)$
- Estimate the probability that the sample mean is larger than 20.

#### Simulation

- Let's first solve by simulation.
- We can use the `rgamma()` function to generate random gamma variables and use `map_dbl()` as earlier to take sample means.

```{r}
B = 1000000
prob5_df = tibble(
  xbar = map_dbl(1:B, ~mean(rgamma(4, 0.9, 0.09))))
```

- Plot the density
- Add a normal curve
- Theory tells us that $\E(\bar{X}) = 10$ and $\SD(\bar{X}) = \frac{\sqrt{0.9}/0.09}{\sqrt{4}} \doteq `r myround(sqrt(0.9)/0.09/2, 2)`$

```{r}
alpha = 0.9
lambda = 0.09
mu = alpha/lambda
sigma = sqrt(alpha) / lambda

ggplot(prob5_df, aes(x=xbar)) +
  geom_density(color = "blue") +
  geom_norm_density(mu, sigma/2, color = "red") +
  geom_norm_fill(mu, sigma/2, a = 20, fill = "red") +
  geom_hline(yintercept = 0) +
  xlab("Sample Mean") +
  ylab("Density") +
  ggtitle("Skewed Gamma Distribution Sample Mean, n=4")
```

- We can see that the simulated density (blue) is not well approximated by the normal curve with the theoretical mean and standard deviation.
- The true probability will be much larger than that we would calculate using the central limit theorem.

##### Simulation-based probability calculation

```{r}
prob5 = prob5_df %>% 
  summarize(p = mean(xbar > 20))

prob5
```

##### Central Limit Theorem

```{r}
mu
sigma

1 - pnorm(20, mu, sigma/sqrt(4))
```

- The CLT value is less than half the simulation-based calcultion.

##### True Value

- Using theory beyond the scope of this course,
we can calculate the exact probability.

```{r}
prob5_exact = 1 - pgamma(20, 4*alpha, 4*lambda)
```

- The simulated value `r myround(prob5, 4)` is very close to the true theoretical value `r myround(prob5_exact, 4)`.

> Simulation can be very accurate without any knowledge of the theory to make exact calculations and without knowing the true mean and standard deviation.

- But, the CLT is handy

> When the sample size $n$ is large enough for a given problem, then a simple area under a normal curve can be very accurate.

### Example 6

- Here is another example with $n=4$ where the central limit theorem is accurate.
- We use the gamma distribution again, but set $\alpha = 9$ and $\lambda = 0.9$ so that the mean $\mu = 10$ again and $\sigma = \sqrt{9}/0.9 \doteq `r myround(3/0.9, 2)`$.
- Find $\prob(\bar{X} > 12)$.

```{r, echo = FALSE}
alpha = 9
lambda = 0.9
mu = alpha/lambda
sigma = sqrt(alpha) / lambda

tibble(
  x = c(0.01, seq(0.1, 20, 0.1)),
  y = dgamma(x, alpha, lambda)) %>% 
ggplot(aes(x=x, y=y)) +
  geom_line(color = "blue") +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  xlab("x") + 
  ylab("Density") +
  ggtitle("Gamma(9, 0.9) Distribution")
```

#### Simulation

```{r}
B = 1000000
prob6_df = tibble(
  xbar = map_dbl(1:B, ~mean(rgamma(4, alpha, lambda))))
```

- Plot the density
- Add a normal curve with the same mean and sd

```{r}
alpha = 9
lambda = 0.9
mu = alpha/lambda
sigma = sqrt(alpha) / lambda

ggplot(prob6_df, aes(x=xbar)) +
  geom_norm_density(mu, sigma/2, color = "red") +
  geom_norm_fill(mu, sigma/2, a = 12, fill = "red") +
  geom_density(color = "blue") +
  geom_hline(yintercept = 0) +
  xlab("Sample Mean") +
  ylab("Density") +
  ggtitle("More Symmetric Gamma Distribution, Sample Mean, n=4")
```

- We can see that the simulated density (blue) is fairly well approximated by the normal curve with the theoretical mean and standard deviation,
even if not perfect, and $n=4$ only!
- The graph shows which intervals the areas are nearly the same and where they may differ by a bit.
- For a moderately large $n$, such as $n=40$, the normal approximation would have been much more accurate for nearly any interval.



##### Comparison

```{r}
prob6 = prob6_df %>% 
  summarize(simulation = mean(xbar > 12),
            clt = 1 - pnorm(12, mu, sigma/sqrt(4)),
            exact = 1 - pgamma(12, 4*alpha, 4*lambda))

prob6
```
